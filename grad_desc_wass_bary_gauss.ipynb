{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9njZBoT7mVAFgpYnz/AYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GHes31415/Research_OT/blob/main/grad_desc_wass_bary_gauss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U5KuIyamuycJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I'm going to try the followin loss funciton for finding the barycenter of Gaussian distributions in the case $n = 2$\n"
      ],
      "metadata": {
        "id": "DBq8i3MPvLE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 30\n",
        "sampling_cov_wish = torch.distributions.wishart.Wishart(n,covariance_matrix=torch.eye(n))\n",
        "sampling_cov_lkj = torch.distributions.lkj_cholesky.LKJCholesky(n,0.5)\n",
        "# sig_1 = sampling_cov.sample()\n",
        "# sig_2 = torch.tensor([[3,1],[1,5]],dtype= torch.float32)#.to(device)\n",
        "# sig_3 = torch.tensor([[5,-4],[-4,6]],dtype= torch.float32)\n",
        "# sig_4 = torch.tensor([[15,-8],[-8,60]],dtype= torch.float32)\n",
        "# sig_5 = torch.tensor([[0.1,0.001],[0.01,1]],dtype= torch.float32)\n",
        "# sig_6 = torch.tensor([[10000,0.001],[0.01,1]],dtype= torch.float32)\n",
        "# sig_7 = torch.tensor([[10000,-0.001],[-0.01,1]],dtype= torch.float32)\n",
        "# cov_mats = [sig_1,sig_2,sig_3,sig_4,sig_5,sig_6,sig_7]\n",
        "k = 9\n",
        "cov_mats_wish = []\n",
        "cov_mats_lkj = []\n",
        "for i in range(k):\n",
        "  L = sampling_cov_lkj.sample()\n",
        "  cov_mats_lkj.append((i+1)*L@L.T)\n",
        "  # L2 = sampling_cov_wish.sample()\n",
        "  # cov_mats_wish.append(L2@L2.T)\n",
        "\n",
        "# print(cov_mats_lkj)\n",
        "\n",
        "# print(cov_mats_wish)"
      ],
      "metadata": {
        "id": "uNKfhzGWu3Sx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for commutativity\n",
        "# torch.norm(sig_1@sig_2-sig_2@sig_1)"
      ],
      "metadata": {
        "id": "y7g8urrPxTp6"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sqrt_cov(sig: torch.tensor, inverse= False):\n",
        "  '''\n",
        "  sig: nxn SPD matrix\n",
        "  '''\n",
        "  D,U = torch.linalg.eig(sig)\n",
        "  if inverse:\n",
        "    return torch.matmul(torch.matmul(U,torch.diag(D**0.5)),torch.linalg.inv(U)).type(torch.float32),torch.matmul(torch.matmul(U,torch.diag(D**(-0.5))),torch.linalg.inv(U)).type(torch.float32)\n",
        "  else:\n",
        "    return torch.matmul(torch.matmul(U,torch.diag(D**0.5)),torch.linalg.inv(U)).type(torch.float32)"
      ],
      "metadata": {
        "id": "ry-hdkZ5yNX_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sqrt_cov_np(sig: np.array, inverse= False):\n",
        "  '''\n",
        "  sig: nxn SPD matrix\n",
        "  '''\n",
        "  D,U = np.linalg.eig(sig)\n",
        "  if inverse:\n",
        "    return np.matmul(np.matmul(U,np.diag(D**0.5)),np.linalg.inv(U)).astype('float64'),np.matmul(np.matmul(U,np.diag(D**(-0.5))),np.linalg.inv(U)).astype('float64')\n",
        "  else:\n",
        "    return np.matmul(np.matmul(U,np.diag(D**0.5)),np.linalg.inv(U)).astype('float64')"
      ],
      "metadata": {
        "id": "l28rPRfff-O7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(theta,cov_mats,k):\n",
        "  '''\n",
        "  theta:    torch.tensor size n^2,\n",
        "            current estimate.\n",
        "  cov_mats: list of size k of tensors with shape nxn,\n",
        "            covariance matrices.\n",
        "  k:        torch.tensor size 1,\n",
        "            number of covariance matrices.\n",
        "  '''\n",
        "  theta_m = theta.reshape((n,n))#.to(device)\n",
        "  sig_theta = torch.matmul(theta_m,theta_m.T)\n",
        "  sum_loss = (wass_distance(sig_theta,cov_mats,k)).sum()\n",
        "\n",
        "\n",
        "\n",
        "  return sum_loss/k\n",
        "\n",
        "def loss2(theta,cov_mats,k):\n",
        "  #Maybe this has to be modified to be squared\n",
        "  sum_loss = (dis_par_to_covs(theta,cov_mats,k)).sum()\n",
        "  return sum_loss/k\n",
        "def loss3(theta,cov_mats,k):\n",
        "  #Maybe this has to be modified to be squared\n",
        "  sum_loss = (dis_par_to_covs_2(theta,cov_mats,k)).sum()\n",
        "  return sum_loss/k"
      ],
      "metadata": {
        "id": "cvV1-5o1v3-m"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dis_par_to_covs(theta,cov_mats,k):\n",
        "  '''\n",
        "  theta:    torch.tensor size n^2,\n",
        "            current estimate.\n",
        "  cov_mats: list of size k of tensors with shape nxn,\n",
        "            covariance matrices.\n",
        "  k:        torch.tensor size 1,\n",
        "            number of covariance matrices.\n",
        "  '''\n",
        "  theta_m = theta.reshape((n,n))#.to(device)\n",
        "  sig_theta = torch.matmul(theta_m,theta_m.T)\n",
        "  sig_theta_sqrt,sig_theta_sqrt_inv = sqrt_cov(sig_theta,True)\n",
        "  # sig_theta_sqrt = sqrt_cov(sig_theta)\n",
        "  # sig_theta_sqrt_inv = torch.linalg.inv(sig_theta_sqrt)\n",
        "  distances = torch.zeros(k)\n",
        "  for i,sig in enumerate(cov_mats):\n",
        "    first_mat = torch.matmul(torch.matmul(sig_theta_sqrt,sig),sig_theta_sqrt)\n",
        "    sqrt_mat = sqrt_cov(first_mat)\n",
        "    second_mat = torch.matmul(sig_theta_sqrt_inv,sqrt_mat)\n",
        "    distances[i] =torch.linalg.matrix_norm(sig_theta_sqrt-second_mat)**2\n",
        "  return distances\n",
        "def dis_par_to_covs_2(theta,cov_mats,k):\n",
        "  '''\n",
        "  theta:    torch.tensor size n^2,\n",
        "            current estimate.\n",
        "  cov_mats: list of size k of tensors with shape nxn,\n",
        "            covariance matrices.\n",
        "  k:        torch.tensor size 1,\n",
        "            number of covariance matrices.\n",
        "  '''\n",
        "  theta_m = theta.reshape((n,n))#.to(device)\n",
        "  sig_theta = torch.matmul(theta_m,theta_m.T)\n",
        "  distances = torch.zeros(k)\n",
        "  for i,sig in enumerate(cov_mats):\n",
        "    sig_sqrt,sig_sqrt_inv = sqrt_cov(sig,True)\n",
        "    first_mat = sqrt_cov(torch.matmul(torch.matmul(sig_sqrt,sig_theta),sig_sqrt))\n",
        "    second_mat = torch.matmul(sig_sqrt_inv,first_mat)\n",
        "    distances[i] =torch.linalg.matrix_norm(sig_sqrt-second_mat)**2\n",
        "  return distances\n",
        "def dis_cov_to_covs(cov,cov_mats,k):\n",
        "  cov_sqrt,cov_sqrt_inv = sqrt_cov(cov,True)\n",
        "  distances = torch.zeros(k)\n",
        "  for i,sig in enumerate(cov_mats):\n",
        "    first_mat = torch.matmul(torch.matmul(cov_sqrt,sig),cov_sqrt)\n",
        "    # print(first_mat)\n",
        "    sqrt_mat = sqrt_cov(first_mat)\n",
        "    second_mat = torch.matmul(cov_sqrt_inv,sqrt_mat)\n",
        "    distances[i] = torch.linalg.matrix_norm(cov_sqrt-second_mat)**2\n",
        "\n",
        "\n",
        "\n",
        "  return distances"
      ],
      "metadata": {
        "id": "ZtQ4HZ4S7LgH"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wass_distance(cov,cov_mats,k):\n",
        "  cov_sqrt = sqrt_cov(cov)\n",
        "  distances = torch.zeros(k)\n",
        "  for i,sig in enumerate(cov_mats):\n",
        "    mat = sqrt_cov(torch.matmul(torch.matmul(cov_sqrt,sig),cov_sqrt))\n",
        "    distances[i] = torch.trace(cov+sig-2*mat)\n",
        "\n",
        "  return distances\n"
      ],
      "metadata": {
        "id": "2PsPquTIxPmC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# theta = torch.randn(n**2,requires_grad=True)#.to(device)\n",
        "# optimizer = torch.optim.Adam([theta],lr = lr)"
      ],
      "metadata": {
        "id": "o_kAmRpp0gl6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def training(theta,lr,n_iters,loss,cov_mats):\n",
        "  increment = 1\n",
        "  for epoch in range(n_iters):\n",
        "\n",
        "    # compute loss and backpropagate\n",
        "    l = loss(theta,cov_mats,k)\n",
        "    l.backward()\n",
        "    d_theta = theta.grad\n",
        "    # update theta gradient descent\n",
        "    with torch.no_grad():\n",
        "      theta -= lr*d_theta\n",
        "    if torch.norm(lr*d_theta)<10**(-6):\n",
        "      print(epoch,torch.norm(lr*d_theta))\n",
        "      return theta\n",
        "    if epoch%500 == 0:\n",
        "      print(epoch,torch.norm(lr*d_theta))\n",
        "    # zero graidents\n",
        "    theta.grad.zero_()\n",
        "  return theta\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WVvZ44B1w-C"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = torch.tensor(.04)#.to(device)\n",
        "n_iters = 5000"
      ],
      "metadata": {
        "id": "oHKkjFVcgPde"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = time.time()\n",
        "theta = torch.randn(n**2,requires_grad=True)#.to(device)\n",
        "theta_w2 = training(theta,lr,n_iters,loss,cov_mats_lkj)\n",
        "print(time.time()-t1)"
      ],
      "metadata": {
        "id": "p6xGfYHf-LDm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "bd8b7b77-ae3b-437f-9440-a5adacccd9fc"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.4529)\n",
            "500 tensor(0.0002)\n",
            "1000 tensor(2.9104e-05)\n",
            "1500 tensor(1.2396e-05)\n",
            "2000 tensor(1.2654e-05)\n",
            "2500 tensor(2.5920e-05)\n",
            "3000 tensor(7.4809e-06)\n",
            "3500 tensor(1.7656e-05)\n",
            "4000 tensor(1.1731e-05)\n",
            "4500 tensor(7.6571e-06)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-35a46f1c1d9e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtheta_w2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcov_mats_lkj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-ad43b2d9a378>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(theta, lr, n_iters, loss, cov_mats)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# compute loss and backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcov_mats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0md_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# update theta gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = time.time()\n",
        "theta = torch.randn(n**2,requires_grad=True)#.to(device)\n",
        "theta_par = training(theta,lr,n_iters,loss2,cov_mats_lkj)\n",
        "print(time.time()-t1)"
      ],
      "metadata": {
        "id": "8qjvon3w-ZRM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "5497da7f-b0fe-4417-ca56-dc07c2d9498d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(1.7691)\n",
            "500 tensor(2.3830e-05)\n",
            "1000 tensor(0.0002)\n",
            "1500 tensor(3.7852e-05)\n",
            "2000 tensor(6.0572e-05)\n",
            "2500 tensor(4.6527e-05)\n",
            "3000 tensor(4.6324e-05)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-23116382160b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtheta_par\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcov_mats_lkj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-ad43b2d9a378>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(theta, lr, n_iters, loss, cov_mats)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# compute loss and backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcov_mats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0md_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# update theta gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = time.time()\n",
        "theta = torch.randn(n**2,requires_grad=True)#.to(device)\n",
        "theta_par = training(theta,lr,n_iters,loss3,cov_mats_lkj)\n",
        "print(time.time()-t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "tyzXpvqGNuSt",
        "outputId": "4613e86a-17c4-44eb-ebff-08d89d21f515"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.0900)\n",
            "1000 tensor(3.9338e-05)\n",
            "2000 tensor(6.7658e-05)\n",
            "3000 tensor(3.4791e-05)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-7791143d2f14>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtheta_par\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcov_mats_lkj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-4d20f2a99894>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(theta, lr, n_iters, loss, cov_mats)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# compute loss and backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcov_mats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0md_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# update theta gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fixed_point_it_sol(n,cov_mat,k,n_iters):\n",
        "  sig = torch.randn(n,n)\n",
        "  sig = torch.matmul(sig,sig.T)\n",
        "  sig_sqrt,sig_sqrt_inv = sqrt_cov(sig,True)\n",
        "  increase = 1\n",
        "  # counter = 0\n",
        "  for epoch in range(n_iters):\n",
        "    # print(sig)\n",
        "\n",
        "    sum_mats = torch.zeros_like(sig)\n",
        "    for cov in cov_mat:\n",
        "      sum_mats += sqrt_cov(torch.matmul(torch.matmul(sig_sqrt,cov),sig_sqrt))/k\n",
        "    mat = torch.matmul(sum_mats,sum_mats)\n",
        "    sig_next = torch.matmul(torch.matmul(sig_sqrt_inv,mat),sig_sqrt_inv)\n",
        "    # print(f'sig next{sig_next}')\n",
        "    increase = torch.linalg.matrix_norm(sig-sig_next)\n",
        "    sig = sig_next.clone()\n",
        "    sig_sqrt,sig_sqrt_inv = sqrt_cov(sig,True)\n",
        "    if increase<10**(-6):\n",
        "      print(epoch,increase)\n",
        "      return sig\n",
        "    if epoch%1000==0:\n",
        "      print(epoch,increase)\n",
        "  print(epoch)\n",
        "  return sig"
      ],
      "metadata": {
        "id": "hT7DYxNR0lmx"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fixed_point_it_sol_np(n,cov_mat,k,n_iters):\n",
        "  sig = np.random.normal( size = (n,n))\n",
        "  sig = np.matmul(sig,sig.T)\n",
        "  sig_sqrt,sig_sqrt_inv = sqrt_cov_np(sig,True)\n",
        "  increase = 1\n",
        "  # counter = 0\n",
        "  for epoch in range(n_iters):\n",
        "    # print(sig)\n",
        "\n",
        "    sum_mats = np.zeros_like(sig)\n",
        "    for cov in cov_mat:\n",
        "      sum_mats += sqrt_cov_np(np.matmul(np.matmul(sig_sqrt,cov),sig_sqrt))/k\n",
        "    mat = np.matmul(sum_mats,sum_mats)\n",
        "    sig_next = np.matmul(np.matmul(sig_sqrt_inv,mat),sig_sqrt_inv)\n",
        "    # print(f'sig next{sig_next}')\n",
        "    increase = np.linalg.norm(sig-sig_next)\n",
        "    sig = sig_next.copy()\n",
        "    sig_sqrt,sig_sqrt_inv = sqrt_cov_np(sig,True)\n",
        "    if increase<10**(-10):\n",
        "      print(epoch,increase)\n",
        "      return sig\n",
        "    if epoch%1000==0:\n",
        "      print(epoch,increase)\n",
        "  print(epoch)\n",
        "  return sig"
      ],
      "metadata": {
        "id": "vba9-x6Zh6CF"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = time.time()\n",
        "sig = fixed_point_it_sol_np(n,cov_mats_lkj,k,n_iters)\n",
        "print(time.time()-t1)"
      ],
      "metadata": {
        "id": "uMq6_OI8lviJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30aa2250-2a03-464c-f75b-7b5c3c581488"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 228.98437652264812\n",
            "45 9.519822041123744e-11\n",
            "0.5668394565582275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJlhWNoLAxiV"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}